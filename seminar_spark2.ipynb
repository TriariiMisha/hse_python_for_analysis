{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P620u1HUZFG9"
   },
   "source": [
    "## Spark\n",
    "\n",
    "Схема данных, датасеты\n",
    "\n",
    "В наборе данных Iris:\n",
    "\t•\tСепал (sepal) – это внешняя, защитная часть цветка, которая закрывает и оберегает его до распускания.\n",
    "\t•\tЛепесток (petal) – это внутренняя, часто яркая часть цветка, предназначенная для привлечения опылителей.\n",
    "\n",
    "Spark DataFrame - https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q4IEbzSRWQgK",
    "outputId": "84a25037-259a-48ec-c562-f51bb93dbf4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный датасет Iris:\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|    species|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "|         5.1|        3.5|         1.4|        0.2|Iris-setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
      "+------------+-----------+------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast, col\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType\n",
    "\n",
    "# 1. Создаем Spark сессию\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Пример: датасет из нескольких частей\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Задаем схему для исходного датасета Iris\n",
    "iris_schema = StructType([\n",
    "    StructField(\"sepal_length\", DoubleType(), True),  # название поля, тип, nullable\n",
    "    StructField(\"sepal_width\", DoubleType(), True),\n",
    "    StructField(\"petal_length\", DoubleType(), True),\n",
    "    StructField(\"petal_width\", DoubleType(), True),\n",
    "    StructField(\"species\", StringType(), True)\n",
    "])\n",
    "\n",
    "# 2. Скачиваем датасет Iris, если файл отсутствует\n",
    "iris_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "iris_file = \"iris.csv\"\n",
    "if not os.path.exists(iris_file):\n",
    "    print(\"Скачиваем датасет Iris...\")\n",
    "    response = requests.get(iris_url)\n",
    "    with open(iris_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Скачивание завершено!\")\n",
    "\n",
    "# 3. Читаем CSV-файл в DataFrame с заданной схемой\n",
    "# header - false - не читать заголовок\n",
    "df_iris = spark.read.option(\"header\", \"false\") \\\n",
    "                    .schema(iris_schema) \\\n",
    "                    .csv(iris_file)\n",
    "# Убираем пустые строки (если они есть)\n",
    "df_iris = df_iris.filter(col(\"species\") != \"\")\n",
    "print(\"Исходный датасет Iris:\")\n",
    "df_iris.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XMS8LRBkZNoU",
    "outputId": "b1a9c21f-47a0-4ea6-a377-c01a64ecae34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет Iris записан в виде нескольких частей в папку 'data/iris_parts'.\n",
      "Схема объединенного датасета из нескольких частей:\n",
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n",
      "+------------+-----------+------------+-----------+--------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|       species|\n",
      "+------------+-----------+------------+-----------+--------------+\n",
      "|         6.3|        3.3|         6.0|        2.5|Iris-virginica|\n",
      "|         5.8|        2.7|         5.1|        1.9|Iris-virginica|\n",
      "|         7.1|        3.0|         5.9|        2.1|Iris-virginica|\n",
      "|         6.3|        2.9|         5.6|        1.8|Iris-virginica|\n",
      "|         6.5|        3.0|         5.8|        2.2|Iris-virginica|\n",
      "+------------+-----------+------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Записываем датасет Iris на диск, разделяя его по столбцу \"species\"\n",
    "output_dir = \"data/iris_parts\"\n",
    "df_iris.write.mode(\"overwrite\").partitionBy(\"species\").csv(output_dir, header=True)\n",
    "print(f\"Датасет Iris записан в виде нескольких частей в папку '{output_dir}'.\")\n",
    "\n",
    "# 5. Читаем объединенный датасет из нескольких частей\n",
    "# Важно: читаем корневую папку без подстановочного шаблона, чтобы Spark обнаружил колонку partition (\"species\")\n",
    "# При этом указываем схему для остальных колонок\n",
    "data_schema = StructType([\n",
    "    StructField(\"sepal_length\", DoubleType(), True),\n",
    "    StructField(\"sepal_width\", DoubleType(), True),\n",
    "    StructField(\"petal_length\", DoubleType(), True),\n",
    "    StructField(\"petal_width\", DoubleType(), True)\n",
    "])\n",
    "df_parts = spark.read.option(\"header\", \"true\").schema(data_schema).csv(output_dir)\n",
    "print(\"Схема объединенного датасета из нескольких частей:\")\n",
    "df_parts.printSchema()\n",
    "df_parts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gGEGqTQ7Zd4f",
    "outputId": "76a1dd0e-47dd-4383-dd95-70899965bec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Справочный датасет:\n",
      "+---------------+------------------+\n",
      "|        species|       description|\n",
      "+---------------+------------------+\n",
      "|    Iris-setosa|Маленькие лепестки|\n",
      "|Iris-versicolor|  Средние лепестки|\n",
      "| Iris-virginica|  Большие лепестки|\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Создаем справочный DataFrame с информацией о видах\n",
    "lookup_data = [\n",
    "    (\"Iris-setosa\", \"Маленькие лепестки\"),\n",
    "    (\"Iris-versicolor\", \"Средние лепестки\"),\n",
    "    (\"Iris-virginica\", \"Большие лепестки\")\n",
    "]\n",
    "df_lookup = spark.createDataFrame(lookup_data, [\"species\", \"description\"])\n",
    "print(\"Справочный датасет:\")\n",
    "df_lookup.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTmW10dSWYQq"
   },
   "source": [
    "https://medium.com/@Prashank.jauhari/understanding-broadcast-variables-in-apache-spark-8233d35726fc#:~:text=Broadcast%20Variables%20in%20Spark%20allow,RDD%20(Resilient%20Distributed%20Dataset).  Больше про broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zcb9mnhZjgm",
    "outputId": "61e07358-dd36-4306-840c-0291cd4478e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат broadcast join:\n",
      "+--------------+------------+-----------+------------+-----------+----------------+\n",
      "|       species|sepal_length|sepal_width|petal_length|petal_width|     description|\n",
      "+--------------+------------+-----------+------------+-----------+----------------+\n",
      "|Iris-virginica|         6.3|        3.3|         6.0|        2.5|Большие лепестки|\n",
      "|Iris-virginica|         5.8|        2.7|         5.1|        1.9|Большие лепестки|\n",
      "|Iris-virginica|         7.1|        3.0|         5.9|        2.1|Большие лепестки|\n",
      "|Iris-virginica|         6.3|        2.9|         5.6|        1.8|Большие лепестки|\n",
      "|Iris-virginica|         6.5|        3.0|         5.8|        2.2|Большие лепестки|\n",
      "+--------------+------------+-----------+------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Выполняем broadcast join между объединенным датасетом и справочным\n",
    "df_joined = df_parts.join(broadcast(df_lookup), on=\"species\", how=\"left\")\n",
    "print(\"Результат broadcast join:\")\n",
    "df_joined.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMvGz14vZmcx",
    "outputId": "679545ba-1940-43cd-a51d-50c8e69b0208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество партиций после репартитионирования: 20\n"
     ]
    }
   ],
   "source": [
    "# 8. Демонстрируем репартитионирование объединенного датасета.\n",
    "df_parts = df_parts.withColumn(\"sepal_int\", col(\"sepal_length\").cast(\"integer\"))\n",
    "df_repartitioned = df_parts.repartition(20, col(\"sepal_int\"))\n",
    "print(\"Количество партиций после репартитионирования:\", df_repartitioned.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIOemVTMacxi"
   },
   "source": [
    "Spark UI https://spark.apache.org/docs/3.5.3/web-ui.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xHAkqBhbbbp"
   },
   "source": [
    "Что такое партиционирование?\n",
    "\n",
    "В Apache Spark данные распределяются между узлами кластера в виде RDD (резилиентных распределённых наборов данных) или DataFrame. Каждый такой набор данных разбивается на «партиции» – логические блоки данных, которые обрабатываются параллельно. Количество партиций напрямую влияет на уровень параллелизма: каждая партиция обрабатывается отдельной задачей (task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZskL7QZ-Zpbp",
    "outputId": "df1c613d-8043-473d-d34f-b4632ae8be36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет сохранен в формате Parquet в папку 'output/iris_parquet_multi'.\n",
      "Восстановленный DataFrame из Parquet:\n",
      "+------------+-----------+------------+-----------+---------+--------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|sepal_int|species       |\n",
      "+------------+-----------+------------+-----------+---------+--------------+\n",
      "|6.3         |3.3        |6.0         |2.5        |6        |Iris-virginica|\n",
      "|6.3         |2.9        |5.6         |1.8        |6        |Iris-virginica|\n",
      "|6.5         |3.0        |5.8         |2.2        |6        |Iris-virginica|\n",
      "|6.7         |2.5        |5.8         |1.8        |6        |Iris-virginica|\n",
      "|6.5         |3.2        |5.1         |2.0        |6        |Iris-virginica|\n",
      "+------------+-----------+------------+-----------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Количество партиций во восстановленном DataFrame: 10\n"
     ]
    }
   ],
   "source": [
    "# 9. Сохраняем репартитиционированный датасет в формате Parquet с разделением по столбцу \"species\"\n",
    "parquet_output = \"output/iris_parquet_multi\"\n",
    "df_repartitioned.write.mode(\"overwrite\").partitionBy(\"species\").parquet(parquet_output)\n",
    "print(f\"Датасет сохранен в формате Parquet в папку '{parquet_output}'.\")\n",
    "\n",
    "# 10. Читаем сохраненный датасет из Parquet\n",
    "df_restored = spark.read.parquet(parquet_output)\n",
    "print(\"Восстановленный DataFrame из Parquet:\")\n",
    "df_restored.show(5, truncate=False)\n",
    "print(\"Количество партиций во восстановленном DataFrame:\", df_restored.rdd.getNumPartitions())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC7GjZ3wbxae"
   },
   "source": [
    "Parquet - колоночный формат данных\n",
    "https://aws.amazon.com/ru/compare/the-difference-between-olap-and-oltp/\n",
    "\n",
    "https://softwareengineeringdaily.com/2017/01/13/columnar-data-apache-arrow-and-parquet-with-julien-le-dem-and-jacques-nadeau/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PPY4pBH5a_S4",
    "outputId": "a9aff3ec-3684-4ab6-814b-d223682fa0b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Описательная статистика набора данных:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/01 09:54:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|summary|      sepal_length|        sepal_width|      petal_length|       petal_width|       species|\n",
      "+-------+------------------+-------------------+------------------+------------------+--------------+\n",
      "|  count|               150|                150|               150|               150|           150|\n",
      "|   mean| 5.843333333333335| 3.0540000000000007|3.7586666666666693|1.1986666666666672|          NULL|\n",
      "| stddev|0.8280661279778637|0.43359431136217375| 1.764420419952262|0.7631607417008414|          NULL|\n",
      "|    min|               4.3|                2.0|               1.0|               0.1|   Iris-setosa|\n",
      "|    max|               7.9|                4.4|               6.9|               2.5|Iris-virginica|\n",
      "+-------+------------------+-------------------+------------------+------------------+--------------+\n",
      "\n",
      "Взаимные корреляции между признаками:\n",
      "Корреляция между sepal_length и sepal_width: -0.109\n",
      "Корреляция между sepal_length и petal_length: 0.872\n",
      "Корреляция между sepal_length и petal_width: 0.818\n",
      "Корреляция между sepal_width и petal_length: -0.421\n",
      "Корреляция между sepal_width и petal_width: -0.357\n",
      "Корреляция между petal_length и petal_width: 0.963\n"
     ]
    }
   ],
   "source": [
    "# Предположим, что мы продолжаем работать с исходным DataFrame df_iris,\n",
    "# который был создан ранее.\n",
    "\n",
    "# 1. Выведем описательную статистику для числовых столбцов\n",
    "print(\"Описательная статистика набора данных:\")\n",
    "df_iris.describe().show()\n",
    "\n",
    "# 2. Вычислим взаимные корреляции между числовыми столбцами\n",
    "numeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "print(\"Взаимные корреляции между признаками:\")\n",
    "for i in range(len(numeric_cols)):\n",
    "    for j in range(i+1, len(numeric_cols)):\n",
    "        corr_val = df_iris.stat.corr(numeric_cols[i], numeric_cols[j])\n",
    "        print(f\"Корреляция между {numeric_cols[i]} и {numeric_cols[j]}: {corr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7aIEi6jbkq4",
    "outputId": "31159f54-0ae6-494c-d7a9-b736b44e49ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/01 09:58:41 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры прогнозов:\n",
      "+---------------+-----+----------+-----------------------------------------------------------------+\n",
      "|species        |label|prediction|probability                                                      |\n",
      "+---------------+-----+----------+-----------------------------------------------------------------+\n",
      "|Iris-setosa    |2.0  |2.0       |[4.599705793769882E-19,4.671058980629929E-4,0.9995328941019369]  |\n",
      "|Iris-setosa    |2.0  |2.0       |[2.749813503816678E-19,5.205764724954879E-4,0.9994794235275045]  |\n",
      "|Iris-setosa    |2.0  |2.0       |[3.569703930131562E-22,2.6874378001183374E-5,0.9999731256219988] |\n",
      "|Iris-setosa    |2.0  |2.0       |[2.3019756241931815E-19,7.122838327080171E-4,0.999287716167292]  |\n",
      "|Iris-setosa    |2.0  |2.0       |[6.760792642654331E-18,0.003634416180262674,0.9963655838197374]  |\n",
      "|Iris-setosa    |2.0  |2.0       |[3.0951077142726367E-19,7.033173666718958E-4,0.9992966826333282] |\n",
      "|Iris-setosa    |2.0  |2.0       |[2.0653966381466125E-18,0.0012527773008829915,0.998747222699117] |\n",
      "|Iris-setosa    |2.0  |2.0       |[1.2731552630108747E-18,0.004158347083545633,0.9958416529164543] |\n",
      "|Iris-setosa    |2.0  |2.0       |[1.2731552630108747E-18,0.004158347083545633,0.9958416529164543] |\n",
      "|Iris-versicolor|1.0  |1.0       |[5.036035045104302E-6,0.9888925431841755,0.011102420780779283]   |\n",
      "|Iris-setosa    |2.0  |2.0       |[4.639111345054812E-17,0.017081697444595003,0.9829183025554049]  |\n",
      "|Iris-setosa    |2.0  |2.0       |[1.516317665815031E-17,0.0027614803579686753,0.9972385196420314] |\n",
      "|Iris-setosa    |2.0  |2.0       |[1.8300547398033046E-19,7.504672987933936E-4,0.9992495327012065] |\n",
      "|Iris-setosa    |2.0  |2.0       |[1.0707066076578185E-16,0.002281598638039378,0.9977184013619604] |\n",
      "|Iris-versicolor|1.0  |1.0       |[1.7828325872363246E-6,0.9706735652353936,0.029324651932019087]  |\n",
      "|Iris-setosa    |2.0  |2.0       |[6.429610214366992E-19,0.002644660860961037,0.9973553391390388]  |\n",
      "|Iris-setosa    |2.0  |2.0       |[1.22198206851475E-19,0.001262209393422621,0.9987377906065774]   |\n",
      "|Iris-setosa    |2.0  |2.0       |[1.9795713885045635E-20,3.583214715669767E-4,0.999641678528433]  |\n",
      "|Iris-setosa    |2.0  |2.0       |[5.378267138884114E-19,0.0036157532325869993,0.9963842467674129] |\n",
      "|Iris-setosa    |2.0  |2.0       |[3.6232845373338956E-19,0.0025359263918943785,0.9974640736081056]|\n",
      "+---------------+-----+----------+-----------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Точность модели: 0.978\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# 1. Преобразуем столбец species в числовой индекс (label)\n",
    "indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "\n",
    "# 2. Собираем числовые признаки в единый столбец \"features\"\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features\")\n",
    "\n",
    "# 3. Создаем классификатор: логистическую регрессию\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# 4. Формируем Pipeline, объединяя этапы преобразования и классификатора\n",
    "pipeline = Pipeline(stages=[indexer, assembler, lr])\n",
    "\n",
    "# 5. Разбиваем данные на обучающую (70%) и тестовую (30%) выборки\n",
    "train_df, test_df = df_iris.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# 6. Обучаем модель\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# 7. Делаем прогнозы на тестовой выборке\n",
    "predictions = model.transform(test_df)\n",
    "print(\"Примеры прогнозов:\")\n",
    "predictions.select(\"species\", \"label\", \"prediction\", \"probability\").show(20, truncate=False)\n",
    "\n",
    "# 8. Оцениваем точность модели\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Точность модели: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUOP-N1NcaSo"
   },
   "source": [
    "## Задание - Эксперименты с настройками Spark\n",
    "\n",
    "1.\tИзменение числа партиций:\n",
    "  -\tПроведите эксперименты с разным количеством партиций (например, 10, 20, 50) для одного и того же датасета.\n",
    "  -\tИзмерьте время выполнения агрегационных операций (например, groupBy и count) при разном числе партиций.\n",
    "  -\tСравните и обсудите результаты.\n",
    "2.\tПроверка влияния параметров broadcast:\n",
    "  -\tИзмените значение параметра spark.sql.autoBroadcastJoinThreshold и выполните join двух DataFrame.\n",
    "  -\tПроанализируйте, как изменяется план выполнения и время выполнения join.\n",
    "\n",
    "  https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page - адрес датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDTiLosCgJ6t",
    "outputId": "4668231d-d644-4045-b975-65e56ec6a36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-01.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-01.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-02.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-02.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-03.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-03.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-04.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-04.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-05.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-05.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-06.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-06.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-07.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-07.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-08.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-08.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-09.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-09.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-10.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-10.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-11.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-11.csv.gz...\n",
      "Скачивание завершено.\n",
      "Скачивание данных из https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2020-12.csv.gz в data/nyc_taxi/yellow/2020/yellow_tripdata_2020-12.csv.gz...\n",
      "Скачивание завершено.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "import time\n",
    "\n",
    "# Функция для скачивания файла по URL\n",
    "def download_file(url, local_path):\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    print(f\"Скачивание данных из {url} в {local_path}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(local_path, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(\"Скачивание завершено.\")\n",
    "\n",
    "# Скачивание данных Yellow Taxi за 2020 год (месяцы 1 - 12)\n",
    "year = \"2020\"\n",
    "taxi_type = \"yellow\"\n",
    "base_url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download\"\n",
    "\n",
    "for month in range(1, 13):\n",
    "    month_str = f\"{month:02d}\"\n",
    "    url = f\"{base_url}/{taxi_type}/yellow_tripdata_{year}-{month_str}.csv.gz\"\n",
    "    local_path = f\"data/nyc_taxi/{taxi_type}/{year}/yellow_tripdata_{year}-{month_str}.csv.gz\"\n",
    "    download_file(url, local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jRQVNQzcaCM",
    "outputId": "f4d57b03-f8d9-43e3-fe9e-fa1e7706bc39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/01 10:01:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер выборки: 6295\n",
      "\n",
      "--- Эксперимент: Изменение числа партиций и измерение времени агрегирования ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество партиций: 1, время выполнения агрегирования: 5.536 сек\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество партиций: 2, время выполнения агрегирования: 6.463 сек\n",
      "CodeCache: size=131072Kb used=45540Kb max_used=45898Kb free=85531Kb\n",
      " bounds [0x0000000105ecc000, 0x0000000108bdc000, 0x000000010decc000]\n",
      " total_blobs=16424 nmethods=15136 adapters=1198\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество партиций: 10, время выполнения агрегирования: 6.355 сек\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Создаем SparkSession\n",
    "spark = SparkSession.builder.appName(\"Эксперименты с партиционированием и broadcast\").getOrCreate()\n",
    "\n",
    "# Читаем скачанный CSV (с распаковкой, если требуется; в данном случае Spark может читать gzipped файлы напрямую)\n",
    "# df_taxi = spark.read.option(\"header\", \"true\").csv(\"data/nyc_taxi/yellow/2020/*.csv.gz\")\n",
    "df_taxi = spark.read.option(\"header\", \"true\").csv(\"data/nyc_taxi/yellow/2020/yellow_tripdata_2020-01.csv.gz\")\n",
    "\n",
    "# Приведение типов: преобразуем даты и числовые поля\n",
    "df_taxi = df_taxi.withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\"))) \\\n",
    "                 .withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"double\")) \\\n",
    "                 .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"double\"))\n",
    "\n",
    "# Для ускорения экспериментов берем подвыборку данных (например, 0.1% от исходного)\n",
    "df_sampled = df_taxi.sample(withReplacement=False, fraction=0.001, seed=42)\n",
    "print(\"Размер выборки:\", df_sampled.count())\n",
    "\n",
    "#############################################\n",
    "# Эксперимент 1. Изменение числа партиций\n",
    "#############################################\n",
    "\n",
    "partitions_list = [1, 2, 10]\n",
    "aggregation_times = {}\n",
    "\n",
    "print(\"\\n--- Эксперимент: Изменение числа партиций и измерение времени агрегирования ---\")\n",
    "for num_parts in partitions_list:\n",
    "    # Репартитионируем выборку на num_parts партиций\n",
    "    df_repart = df_sampled.repartition(num_parts)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Группируем по дате поездки (приводим datetime к дате) и считаем количество строк\n",
    "    df_agg = df_repart.groupBy(col(\"tpep_pickup_datetime\").cast(\"date\").alias(\"pickup_date\")).count().collect()\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    aggregation_times[num_parts] = elapsed_timeу\n",
    "    print(f\"Количество партиций: {num_parts}, время выполнения агрегирования: {elapsed_time:.3f} сек\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BBAvkyQthdKt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_sampled.repartition(10)\\\n",
    "  .write.mode(\"overwrite\").csv(\"taxi-test\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-Cm9jXDjyJ7"
   },
   "source": [
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.createOrReplaceTempView.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fpy_qjBJi1fv",
    "outputId": "8955f0e0-3489-4afa-9228-021a629bf738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       2| 2020-01-01 00:22:56|  2020-01-01 00:37:03|              2|         7.43|         1|                 N|           7|          56|           2|       22.0|  0.5|    0.5|       0.0|           0|                  0.3|        23.3|                   0|\n",
      "|       1| 2020-01-01 00:34:54|  2020-01-01 00:54:13|              1|         5.40|         1|                 N|          43|         148|           2|       19.5|    3|    0.5|       0.0|           0|                  0.3|        23.3|                 2.5|\n",
      "|       1| 2020-01-01 00:20:19|  2020-01-01 00:28:17|              3|         2.10|         1|                 N|         170|         141|           1|        8.0|    3|    0.5|      2.35|           0|                  0.3|       14.15|                 2.5|\n",
      "|       2| 2020-01-01 00:26:12|  2020-01-01 00:39:28|              1|         5.99|         1|                 N|         170|          33|           1|       18.5|  0.5|    0.5|      6.69|           0|                  0.3|       28.99|                 2.5|\n",
      "|       2| 2020-01-01 00:14:43|  2020-01-01 00:24:56|              1|         2.04|         1|                 N|         238|         166|           1|       10.0|  0.5|    0.5|      3.45|           0|                  0.3|       17.25|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sampled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0yELeHjj-fO",
    "outputId": "97f7a564-8d6b-4e11-f3e3-d2459649d8a7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|VendorID|total_passengers|\n",
      "+--------+----------------+\n",
      "|    NULL|            NULL|\n",
      "|       1|          2434.0|\n",
      "|       2|          6998.0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "df_sampled.groupBy(\"VendorID\")\\\n",
    "          .agg(spark_sum(\"passenger_count\").alias(\"total_passengers\"))\\\n",
    "          .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcND5sSykeAR",
    "outputId": "aac85831-081a-438c-d247-e6428f96af35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+\n",
      "|VendorID|total_passenger_count|\n",
      "+--------+---------------------+\n",
      "|    NULL|                 NULL|\n",
      "|       1|               2434.0|\n",
      "|       2|               6998.0|\n",
      "+--------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_sampled.createOrReplaceTempView(\"trips\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT VendorID, SUM(passenger_count) AS total_passenger_count\n",
    "    FROM trips\n",
    "    GROUP BY VendorID\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXrx6xRDk-XH",
    "outputId": "76a7d8dd-7e6d-409a-fa27-ff7f77f76784"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 84:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+\n",
      "|VendorID|total_passenger_count|\n",
      "+--------+---------------------+\n",
      "|    NULL|                 NULL|\n",
      "|       1|                 2434|\n",
      "|       2|                 6998|\n",
      "+--------+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_sampled.select(\"VendorID\", col(\"passenger_count\").cast(\"int\").alias(\"passenger_count\")) \\\n",
    "  .groupBy(\"VendorID\") \\\n",
    "  .sum(\"passenger_count\") \\\n",
    "  .withColumnRenamed(\"sum(passenger_count)\", \"total_passenger_count\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9417xpz7g8EU"
   },
   "source": [
    "Параметр spark.sql.autoBroadcastJoinThreshold определяет максимальный размер (в байтах) таблицы, которая при выполнении операции join может быть автоматически «транслярована» (broadcast) на все узлы кластера. То есть если размер одной из таблиц меньше установленного порога, Spark автоматически передаст её на каждый executor, чтобы выполнить join локально, что позволяет избежать затратного перемешивания данных (shuffle).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JeHPL_iOgGnz",
    "outputId": "2106c5b8-bc63-4cc9-cb1a-f373efd3cb59",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Эксперимент: Влияние параметра spark.sql.autoBroadcastJoinThreshold на join ---\n",
      "\n",
      "Значение spark.sql.autoBroadcastJoinThreshold = -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения join: 2.350 сек, количество строк: 6295\n",
      "План выполнения join:\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter, [payment_type])\n",
      ":- Sample 0.0, 0.001, false, 42\n",
      ":  +- Project [VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, fare_amount#2148, extra#2103, mta_tax#2104, cast(tip_amount#2105 as double) AS tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      ":     +- Project [VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, cast(fare_amount#2102 as double) AS fare_amount#2148, extra#2103, mta_tax#2104, tip_amount#2105, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      ":        +- Project [VendorID#2092, to_timestamp(tpep_pickup_datetime#2093, None, TimestampType, Some(Europe/Moscow), false) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, fare_amount#2102, extra#2103, mta_tax#2104, tip_amount#2105, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      ":           +- Relation [VendorID#2092,tpep_pickup_datetime#2093,tpep_dropoff_datetime#2094,passenger_count#2095,trip_distance#2096,RatecodeID#2097,store_and_fwd_flag#2098,PULocationID#2099,DOLocationID#2100,payment_type#2101,fare_amount#2102,extra#2103,mta_tax#2104,tip_amount#2105,tolls_amount#2106,improvement_surcharge#2107,total_amount#2108,congestion_surcharge#2109] csv\n",
      "+- Deduplicate [payment_type#2510]\n",
      "   +- Project [payment_type#2510]\n",
      "      +- Sample 0.0, 0.001, false, 42\n",
      "         +- Project [VendorID#2501, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2503, passenger_count#2504, trip_distance#2505, RatecodeID#2506, store_and_fwd_flag#2507, PULocationID#2508, DOLocationID#2509, payment_type#2510, fare_amount#2148, extra#2512, mta_tax#2513, cast(tip_amount#2514 as double) AS tip_amount#2167, tolls_amount#2515, improvement_surcharge#2516, total_amount#2517, congestion_surcharge#2518]\n",
      "            +- Project [VendorID#2501, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2503, passenger_count#2504, trip_distance#2505, RatecodeID#2506, store_and_fwd_flag#2507, PULocationID#2508, DOLocationID#2509, payment_type#2510, cast(fare_amount#2511 as double) AS fare_amount#2148, extra#2512, mta_tax#2513, tip_amount#2514, tolls_amount#2515, improvement_surcharge#2516, total_amount#2517, congestion_surcharge#2518]\n",
      "               +- Project [VendorID#2501, to_timestamp(tpep_pickup_datetime#2502, None, TimestampType, Some(Europe/Moscow), false) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2503, passenger_count#2504, trip_distance#2505, RatecodeID#2506, store_and_fwd_flag#2507, PULocationID#2508, DOLocationID#2509, payment_type#2510, fare_amount#2511, extra#2512, mta_tax#2513, tip_amount#2514, tolls_amount#2515, improvement_surcharge#2516, total_amount#2517, congestion_surcharge#2518]\n",
      "                  +- Relation [VendorID#2501,tpep_pickup_datetime#2502,tpep_dropoff_datetime#2503,passenger_count#2504,trip_distance#2505,RatecodeID#2506,store_and_fwd_flag#2507,PULocationID#2508,DOLocationID#2509,payment_type#2510,fare_amount#2511,extra#2512,mta_tax#2513,tip_amount#2514,tolls_amount#2515,improvement_surcharge#2516,total_amount#2517,congestion_surcharge#2518] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "payment_type: string, VendorID: string, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string, RatecodeID: string, store_and_fwd_flag: string, PULocationID: string, DOLocationID: string, fare_amount: double, extra: string, mta_tax: string, tip_amount: double, tolls_amount: string, improvement_surcharge: string, total_amount: string, congestion_surcharge: string\n",
      "Project [payment_type#2101, VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, fare_amount#2148, extra#2103, mta_tax#2104, tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "+- Join LeftOuter, (payment_type#2101 = payment_type#2510)\n",
      "   :- Sample 0.0, 0.001, false, 42\n",
      "   :  +- Project [VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, fare_amount#2148, extra#2103, mta_tax#2104, cast(tip_amount#2105 as double) AS tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "   :     +- Project [VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, cast(fare_amount#2102 as double) AS fare_amount#2148, extra#2103, mta_tax#2104, tip_amount#2105, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "   :        +- Project [VendorID#2092, to_timestamp(tpep_pickup_datetime#2093, None, TimestampType, Some(Europe/Moscow), false) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, fare_amount#2102, extra#2103, mta_tax#2104, tip_amount#2105, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "   :           +- Relation [VendorID#2092,tpep_pickup_datetime#2093,tpep_dropoff_datetime#2094,passenger_count#2095,trip_distance#2096,RatecodeID#2097,store_and_fwd_flag#2098,PULocationID#2099,DOLocationID#2100,payment_type#2101,fare_amount#2102,extra#2103,mta_tax#2104,tip_amount#2105,tolls_amount#2106,improvement_surcharge#2107,total_amount#2108,congestion_surcharge#2109] csv\n",
      "   +- Deduplicate [payment_type#2510]\n",
      "      +- Project [payment_type#2510]\n",
      "         +- Sample 0.0, 0.001, false, 42\n",
      "            +- Project [VendorID#2501, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2503, passenger_count#2504, trip_distance#2505, RatecodeID#2506, store_and_fwd_flag#2507, PULocationID#2508, DOLocationID#2509, payment_type#2510, fare_amount#2148, extra#2512, mta_tax#2513, cast(tip_amount#2514 as double) AS tip_amount#2167, tolls_amount#2515, improvement_surcharge#2516, total_amount#2517, congestion_surcharge#2518]\n",
      "               +- Project [VendorID#2501, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2503, passenger_count#2504, trip_distance#2505, RatecodeID#2506, store_and_fwd_flag#2507, PULocationID#2508, DOLocationID#2509, payment_type#2510, cast(fare_amount#2511 as double) AS fare_amount#2148, extra#2512, mta_tax#2513, tip_amount#2514, tolls_amount#2515, improvement_surcharge#2516, total_amount#2517, congestion_surcharge#2518]\n",
      "                  +- Project [VendorID#2501, to_timestamp(tpep_pickup_datetime#2502, None, TimestampType, Some(Europe/Moscow), false) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2503, passenger_count#2504, trip_distance#2505, RatecodeID#2506, store_and_fwd_flag#2507, PULocationID#2508, DOLocationID#2509, payment_type#2510, fare_amount#2511, extra#2512, mta_tax#2513, tip_amount#2514, tolls_amount#2515, improvement_surcharge#2516, total_amount#2517, congestion_surcharge#2518]\n",
      "                     +- Relation [VendorID#2501,tpep_pickup_datetime#2502,tpep_dropoff_datetime#2503,passenger_count#2504,trip_distance#2505,RatecodeID#2506,store_and_fwd_flag#2507,PULocationID#2508,DOLocationID#2509,payment_type#2510,fare_amount#2511,extra#2512,mta_tax#2513,tip_amount#2514,tolls_amount#2515,improvement_surcharge#2516,total_amount#2517,congestion_surcharge#2518] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [payment_type#2101, VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, fare_amount#2148, extra#2103, mta_tax#2104, tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "+- Sample 0.0, 0.001, false, 42\n",
      "   +- Project [VendorID#2092, cast(tpep_pickup_datetime#2093 as timestamp) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, cast(fare_amount#2102 as double) AS fare_amount#2148, extra#2103, mta_tax#2104, cast(tip_amount#2105 as double) AS tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "      +- Relation [VendorID#2092,tpep_pickup_datetime#2093,tpep_dropoff_datetime#2094,passenger_count#2095,trip_distance#2096,RatecodeID#2097,store_and_fwd_flag#2098,PULocationID#2099,DOLocationID#2100,payment_type#2101,fare_amount#2102,extra#2103,mta_tax#2104,tip_amount#2105,tolls_amount#2106,improvement_surcharge#2107,total_amount#2108,congestion_surcharge#2109] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [payment_type#2101, VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, fare_amount#2148, extra#2103, mta_tax#2104, tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "+- *(1) Sample 0.0, 0.001, false, 42\n",
      "   +- *(1) Project [VendorID#2092, cast(tpep_pickup_datetime#2093 as timestamp) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, cast(fare_amount#2102 as double) AS fare_amount#2148, extra#2103, mta_tax#2104, cast(tip_amount#2105 as double) AS tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "      +- FileScan csv [VendorID#2092,tpep_pickup_datetime#2093,tpep_dropoff_datetime#2094,passenger_count#2095,trip_distance#2096,RatecodeID#2097,store_and_fwd_flag#2098,PULocationID#2099,DOLocationID#2100,payment_type#2101,fare_amount#2102,extra#2103,mta_tax#2104,tip_amount#2105,tolls_amount#2106,improvement_surcharge#2107,total_amount#2108,congestion_surcharge#2109] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/iadovgopolyi/Desktop/Работа/НИУ ВШЭ/Python для сбора и ана..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:string,tpep_pickup_datetime:string,tpep_dropoff_datetime:string,passenger_count:s...\n",
      "\n",
      "\n",
      "Значение spark.sql.autoBroadcastJoinThreshold = 10485760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время выполнения join: 2.148 сек, количество строк: 6295\n",
      "План выполнения join:\n",
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(LeftOuter, [payment_type])\n",
      ":- Sample 0.0, 0.001, false, 42\n",
      ":  +- Project [VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, fare_amount#2148, extra#2103, mta_tax#2104, cast(tip_amount#2105 as double) AS tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      ":     +- Project [VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, cast(fare_amount#2102 as double) AS fare_amount#2148, extra#2103, mta_tax#2104, tip_amount#2105, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      ":        +- Project [VendorID#2092, to_timestamp(tpep_pickup_datetime#2093, None, TimestampType, Some(Europe/Moscow), false) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, fare_amount#2102, extra#2103, mta_tax#2104, tip_amount#2105, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      ":           +- Relation [VendorID#2092,tpep_pickup_datetime#2093,tpep_dropoff_datetime#2094,passenger_count#2095,trip_distance#2096,RatecodeID#2097,store_and_fwd_flag#2098,PULocationID#2099,DOLocationID#2100,payment_type#2101,fare_amount#2102,extra#2103,mta_tax#2104,tip_amount#2105,tolls_amount#2106,improvement_surcharge#2107,total_amount#2108,congestion_surcharge#2109] csv\n",
      "+- Deduplicate [payment_type#2570]\n",
      "   +- Project [payment_type#2570]\n",
      "      +- Sample 0.0, 0.001, false, 42\n",
      "         +- Project [VendorID#2561, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2563, passenger_count#2564, trip_distance#2565, RatecodeID#2566, store_and_fwd_flag#2567, PULocationID#2568, DOLocationID#2569, payment_type#2570, fare_amount#2148, extra#2572, mta_tax#2573, cast(tip_amount#2574 as double) AS tip_amount#2167, tolls_amount#2575, improvement_surcharge#2576, total_amount#2577, congestion_surcharge#2578]\n",
      "            +- Project [VendorID#2561, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2563, passenger_count#2564, trip_distance#2565, RatecodeID#2566, store_and_fwd_flag#2567, PULocationID#2568, DOLocationID#2569, payment_type#2570, cast(fare_amount#2571 as double) AS fare_amount#2148, extra#2572, mta_tax#2573, tip_amount#2574, tolls_amount#2575, improvement_surcharge#2576, total_amount#2577, congestion_surcharge#2578]\n",
      "               +- Project [VendorID#2561, to_timestamp(tpep_pickup_datetime#2562, None, TimestampType, Some(Europe/Moscow), false) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2563, passenger_count#2564, trip_distance#2565, RatecodeID#2566, store_and_fwd_flag#2567, PULocationID#2568, DOLocationID#2569, payment_type#2570, fare_amount#2571, extra#2572, mta_tax#2573, tip_amount#2574, tolls_amount#2575, improvement_surcharge#2576, total_amount#2577, congestion_surcharge#2578]\n",
      "                  +- Relation [VendorID#2561,tpep_pickup_datetime#2562,tpep_dropoff_datetime#2563,passenger_count#2564,trip_distance#2565,RatecodeID#2566,store_and_fwd_flag#2567,PULocationID#2568,DOLocationID#2569,payment_type#2570,fare_amount#2571,extra#2572,mta_tax#2573,tip_amount#2574,tolls_amount#2575,improvement_surcharge#2576,total_amount#2577,congestion_surcharge#2578] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "payment_type: string, VendorID: string, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: string, passenger_count: string, trip_distance: string, RatecodeID: string, store_and_fwd_flag: string, PULocationID: string, DOLocationID: string, fare_amount: double, extra: string, mta_tax: string, tip_amount: double, tolls_amount: string, improvement_surcharge: string, total_amount: string, congestion_surcharge: string\n",
      "Project [payment_type#2101, VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, fare_amount#2148, extra#2103, mta_tax#2104, tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "+- Join LeftOuter, (payment_type#2101 = payment_type#2570)\n",
      "   :- Sample 0.0, 0.001, false, 42\n",
      "   :  +- Project [VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, fare_amount#2148, extra#2103, mta_tax#2104, cast(tip_amount#2105 as double) AS tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "   :     +- Project [VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, cast(fare_amount#2102 as double) AS fare_amount#2148, extra#2103, mta_tax#2104, tip_amount#2105, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "   :        +- Project [VendorID#2092, to_timestamp(tpep_pickup_datetime#2093, None, TimestampType, Some(Europe/Moscow), false) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, fare_amount#2102, extra#2103, mta_tax#2104, tip_amount#2105, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "   :           +- Relation [VendorID#2092,tpep_pickup_datetime#2093,tpep_dropoff_datetime#2094,passenger_count#2095,trip_distance#2096,RatecodeID#2097,store_and_fwd_flag#2098,PULocationID#2099,DOLocationID#2100,payment_type#2101,fare_amount#2102,extra#2103,mta_tax#2104,tip_amount#2105,tolls_amount#2106,improvement_surcharge#2107,total_amount#2108,congestion_surcharge#2109] csv\n",
      "   +- Deduplicate [payment_type#2570]\n",
      "      +- Project [payment_type#2570]\n",
      "         +- Sample 0.0, 0.001, false, 42\n",
      "            +- Project [VendorID#2561, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2563, passenger_count#2564, trip_distance#2565, RatecodeID#2566, store_and_fwd_flag#2567, PULocationID#2568, DOLocationID#2569, payment_type#2570, fare_amount#2148, extra#2572, mta_tax#2573, cast(tip_amount#2574 as double) AS tip_amount#2167, tolls_amount#2575, improvement_surcharge#2576, total_amount#2577, congestion_surcharge#2578]\n",
      "               +- Project [VendorID#2561, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2563, passenger_count#2564, trip_distance#2565, RatecodeID#2566, store_and_fwd_flag#2567, PULocationID#2568, DOLocationID#2569, payment_type#2570, cast(fare_amount#2571 as double) AS fare_amount#2148, extra#2572, mta_tax#2573, tip_amount#2574, tolls_amount#2575, improvement_surcharge#2576, total_amount#2577, congestion_surcharge#2578]\n",
      "                  +- Project [VendorID#2561, to_timestamp(tpep_pickup_datetime#2562, None, TimestampType, Some(Europe/Moscow), false) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2563, passenger_count#2564, trip_distance#2565, RatecodeID#2566, store_and_fwd_flag#2567, PULocationID#2568, DOLocationID#2569, payment_type#2570, fare_amount#2571, extra#2572, mta_tax#2573, tip_amount#2574, tolls_amount#2575, improvement_surcharge#2576, total_amount#2577, congestion_surcharge#2578]\n",
      "                     +- Relation [VendorID#2561,tpep_pickup_datetime#2562,tpep_dropoff_datetime#2563,passenger_count#2564,trip_distance#2565,RatecodeID#2566,store_and_fwd_flag#2567,PULocationID#2568,DOLocationID#2569,payment_type#2570,fare_amount#2571,extra#2572,mta_tax#2573,tip_amount#2574,tolls_amount#2575,improvement_surcharge#2576,total_amount#2577,congestion_surcharge#2578] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [payment_type#2101, VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, fare_amount#2148, extra#2103, mta_tax#2104, tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "+- Sample 0.0, 0.001, false, 42\n",
      "   +- Project [VendorID#2092, cast(tpep_pickup_datetime#2093 as timestamp) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, cast(fare_amount#2102 as double) AS fare_amount#2148, extra#2103, mta_tax#2104, cast(tip_amount#2105 as double) AS tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "      +- Relation [VendorID#2092,tpep_pickup_datetime#2093,tpep_dropoff_datetime#2094,passenger_count#2095,trip_distance#2096,RatecodeID#2097,store_and_fwd_flag#2098,PULocationID#2099,DOLocationID#2100,payment_type#2101,fare_amount#2102,extra#2103,mta_tax#2104,tip_amount#2105,tolls_amount#2106,improvement_surcharge#2107,total_amount#2108,congestion_surcharge#2109] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [payment_type#2101, VendorID#2092, tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, fare_amount#2148, extra#2103, mta_tax#2104, tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "+- *(1) Sample 0.0, 0.001, false, 42\n",
      "   +- *(1) Project [VendorID#2092, cast(tpep_pickup_datetime#2093 as timestamp) AS tpep_pickup_datetime#2128, tpep_dropoff_datetime#2094, passenger_count#2095, trip_distance#2096, RatecodeID#2097, store_and_fwd_flag#2098, PULocationID#2099, DOLocationID#2100, payment_type#2101, cast(fare_amount#2102 as double) AS fare_amount#2148, extra#2103, mta_tax#2104, cast(tip_amount#2105 as double) AS tip_amount#2167, tolls_amount#2106, improvement_surcharge#2107, total_amount#2108, congestion_surcharge#2109]\n",
      "      +- FileScan csv [VendorID#2092,tpep_pickup_datetime#2093,tpep_dropoff_datetime#2094,passenger_count#2095,trip_distance#2096,RatecodeID#2097,store_and_fwd_flag#2098,PULocationID#2099,DOLocationID#2100,payment_type#2101,fare_amount#2102,extra#2103,mta_tax#2104,tip_amount#2105,tolls_amount#2106,improvement_surcharge#2107,total_amount#2108,congestion_surcharge#2109] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/iadovgopolyi/Desktop/Работа/НИУ ВШЭ/Python для сбора и ана..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:string,tpep_pickup_datetime:string,tpep_dropoff_datetime:string,passenger_count:s...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Эксперимент 2. Проверка влияния параметра broadcast\n",
    "#############################################\n",
    "\n",
    "# Создаем небольшой справочный DataFrame: например, уникальные типы оплаты\n",
    "df_payment = df_sampled.select(\"payment_type\").distinct()\n",
    "\n",
    "thresholds = [-1, 10485760]  # -1: отключение broadcast, 10 МБ – типичное значение\n",
    "\n",
    "print(\"\\n--- Эксперимент: Влияние параметра spark.sql.autoBroadcastJoinThreshold на join ---\")\n",
    "for thresh in thresholds:\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", thresh)\n",
    "    print(f\"\\nЗначение spark.sql.autoBroadcastJoinThreshold = {thresh}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Выполняем join по столбцу \"paymentType\"\n",
    "    df_join = df_sampled.join(df_payment, on=\"payment_type\", how=\"left\")\n",
    "    join_count = df_join.count()\n",
    "    elapsed_join = time.time() - start_time\n",
    "    print(f\"Время выполнения join: {elapsed_join:.3f} сек, количество строк: {join_count}\")\n",
    "\n",
    "    print(\"План выполнения join:\")\n",
    "    df_join.explain(True)\n",
    "\n",
    "# Завершаем работу Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
